#!/usr/bin/env python3
"""
Signal Aggregator V11 - é¡¶çº§é‡åŒ–ä¿¡å·èšåˆå™¨
å¯¹æ ‡ Jump Trading / Wintermute çº§åˆ«

æ ¸å¿ƒèƒ½åŠ›:
1. å¤šæºä¿¡å·èšåˆ
2. å®æ—¶å»é‡
3. ä¼˜å…ˆçº§é˜Ÿåˆ—
4. ä¿¡å·åˆå¹¶
5. æ‰¹é‡æ¨é€
"""

import asyncio
import json
import time
import hashlib
from datetime import datetime, timezone
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Set
from enum import Enum
import heapq

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.logging import get_logger
from core.redis_client import RedisClient
from .alpha_engine import AlphaEngine, AlphaSignal, SignalTier, ActionType

logger = get_logger('signal_aggregator')


@dataclass(order=True)
class PrioritizedSignal:
    """ä¼˜å…ˆçº§ä¿¡å· (ç”¨äºä¼˜å…ˆçº§é˜Ÿåˆ—)"""
    priority: float
    timestamp: float = field(compare=False)
    signal: AlphaSignal = field(compare=False)


class SignalAggregator:
    """
    é¡¶çº§é‡åŒ–ä¿¡å·èšåˆå™¨
    
    ç‰¹æ€§:
    - å¤šæºèšåˆ: åˆå¹¶åŒä¸€å¸ç§çš„å¤šæºä¿¡å·
    - ä¼˜å…ˆçº§é˜Ÿåˆ—: Tier-S > Tier-A > Tier-B
    - å®æ—¶å»é‡: 5åˆ†é’Ÿçª—å£å†…å»é‡
    - æ™ºèƒ½åˆå¹¶: ç›¸åŒå¸ç§ä¿¡å·åˆå¹¶å¢å¼º
    """
    
    def __init__(self, redis: Optional[RedisClient] = None):
        self.redis = redis or RedisClient.from_env()
        self.alpha_engine = AlphaEngine(redis=self.redis)
        
        # ä¼˜å…ˆçº§é˜Ÿåˆ—
        self.signal_queue: List[PrioritizedSignal] = []
        
        # èšåˆçŠ¶æ€
        self.symbol_signals: Dict[str, List[AlphaSignal]] = defaultdict(list)
        self.symbol_best_signal: Dict[str, AlphaSignal] = {}
        self.processed_hashes: Set[str] = set()
        
        # é…ç½®
        self.config = {
            'aggregation_window': 30,      # èšåˆæ—¶é—´çª—å£ 30ç§’
            'dedup_window': 300,           # å»é‡çª—å£ 5åˆ†é’Ÿ
            'max_queue_size': 1000,        # æœ€å¤§é˜Ÿåˆ—å¤§å°
            'batch_size': 10,              # æ‰¹é‡å¤„ç†å¤§å°
            'flush_interval': 5,           # åˆ·æ–°é—´éš”
        }
        
        # ç»Ÿè®¡
        self.stats = {
            'events_received': 0,
            'signals_generated': 0,
            'signals_merged': 0,
            'signals_output': 0,
            'tier_s_output': 0,
            'tier_a_output': 0,
            'duplicates_filtered': 0,
        }
        
        logger.info("ğŸ“¡ Signal Aggregator V11 åˆå§‹åŒ–å®Œæˆ")
    
    def _get_priority(self, signal: AlphaSignal) -> float:
        """è®¡ç®—ä¿¡å·ä¼˜å…ˆçº§ (è¶Šå°è¶Šä¼˜å…ˆ)"""
        tier_priority = {
            SignalTier.TIER_S: 0,
            SignalTier.TIER_A: 100,
            SignalTier.TIER_B: 200,
            SignalTier.TIER_C: 300,
            SignalTier.NOISE: 999,
        }
        
        base = tier_priority.get(signal.tier, 500)
        
        # åˆ†æ•°è°ƒæ•´ (åˆ†æ•°è¶Šé«˜ä¼˜å…ˆçº§è¶Šé«˜)
        score_adjustment = -signal.total_score
        
        # é¦–å‘ä¼˜åŠ¿
        first_bonus = -50 if signal.first_seen else 0
        
        # å¤šæºç¡®è®¤ä¼˜åŠ¿
        multi_source_bonus = -signal.exchange_count * 10
        
        return base + score_adjustment + first_bonus + multi_source_bonus
    
    def _get_signal_hash(self, signal: AlphaSignal) -> str:
        """ç”Ÿæˆä¿¡å·å“ˆå¸Œ (ç”¨äºå»é‡)"""
        key = f"{signal.symbol}|{signal.classified_source}|{signal.exchange}"
        return hashlib.md5(key.encode()).hexdigest()[:16]
    
    def _should_merge(self, existing: AlphaSignal, new: AlphaSignal) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶ä¿¡å·"""
        # åŒä¸€å¸ç§, ä¸åŒæ¥æº
        if existing.symbol != new.symbol:
            return False
        
        # æ—¶é—´çª—å£å†…
        if new.timestamp - existing.timestamp > self.config['aggregation_window']:
            return False
        
        # ä¸åŒæ¥æºæˆ–äº¤æ˜“æ‰€
        return (
            existing.classified_source != new.classified_source or
            existing.exchange != new.exchange
        )
    
    def _merge_signals(self, signals: List[AlphaSignal]) -> AlphaSignal:
        """åˆå¹¶å¤šä¸ªä¿¡å·ä¸ºä¸€ä¸ªå¢å¼ºä¿¡å·"""
        if len(signals) == 1:
            return signals[0]
        
        # æŒ‰åˆ†æ•°æ’åº, å–æœ€é«˜åˆ†çš„ä½œä¸ºåŸºç¡€
        signals = sorted(signals, key=lambda s: s.total_score, reverse=True)
        best = signals[0]
        
        # åˆå¹¶æ¥æºå’Œäº¤æ˜“æ‰€
        all_exchanges = set()
        all_sources = set()
        for s in signals:
            all_exchanges.update(s.exchanges)
            all_sources.add(s.classified_source)
        
        # è®¡ç®—åˆå¹¶åçš„åˆ†æ•° (å¤šæºåŠ æˆ)
        multi_bonus = min(len(all_sources) * 10, 40)
        exchange_bonus = min(len(all_exchanges) * 5, 25)
        
        merged_score = best.total_score + multi_bonus + exchange_bonus
        
        # å‡çº§ç­‰çº§
        if best.tier == SignalTier.TIER_A and len(all_exchanges) >= 2:
            tier = SignalTier.TIER_S
            action = ActionType.IMMEDIATE_BUY
            trigger_reason = f"å¤šæºå‡çº§({len(all_sources)}æº,{len(all_exchanges)}æ‰€)"
        elif best.tier == SignalTier.TIER_B and len(all_sources) >= 2:
            tier = SignalTier.TIER_A
            action = ActionType.QUICK_BUY
            trigger_reason = f"å¤šæºå‡çº§({len(all_sources)}æº)"
        else:
            tier = best.tier
            action = best.action
            trigger_reason = best.trigger_reason + f"+{len(signals)-1}æº"
        
        # åˆ›å»ºåˆå¹¶ä¿¡å·
        merged = AlphaSignal(
            id=best.id,
            symbol=best.symbol,
            symbols=list(set(s for sig in signals for s in sig.symbols)),
            tier=tier,
            action=action,
            total_score=round(merged_score, 1),
            source_score=best.source_score,
            exchange_score=best.exchange_score,
            timing_score=best.timing_score,
            volume_score=best.volume_score,
            sentiment_score=best.sentiment_score,
            multi_source_bonus=round(multi_bonus + exchange_bonus, 1),
            source=best.source,
            classified_source=best.classified_source,
            exchange=best.exchange,
            exchanges=list(all_exchanges),
            source_count=len(all_sources),
            exchange_count=len(all_exchanges),
            timestamp=best.timestamp,
            first_seen=best.first_seen,
            latency_ms=best.latency_ms,
            raw_text=best.raw_text,
            contract_address=best.contract_address or next((s.contract_address for s in signals if s.contract_address), None),
            chain=best.chain or next((s.chain for s in signals if s.chain), None),
            market_cap=best.market_cap,
            volume_24h=best.volume_24h,
            price_change_1h=best.price_change_1h,
            trigger_reason=trigger_reason,
            confidence=min(1.0, best.confidence + 0.1 * (len(signals) - 1)),
        )
        
        self.stats['signals_merged'] += 1
        logger.info(f"ğŸ”— ä¿¡å·åˆå¹¶: {best.symbol} | {len(signals)}ä¸ªä¿¡å· -> æ€»åˆ†{merged_score:.0f}")
        
        return merged
    
    async def process_event(self, event: dict) -> Optional[AlphaSignal]:
        """
        å¤„ç†åŸå§‹äº‹ä»¶
        
        Returns:
            AlphaSignal æˆ– None
        """
        self.stats['events_received'] += 1
        
        # Alpha Engine å¤„ç†
        signal = await self.alpha_engine.process_event(event)
        
        if signal is None:
            return None
        
        self.stats['signals_generated'] += 1
        
        # å»é‡
        sig_hash = self._get_signal_hash(signal)
        if sig_hash in self.processed_hashes:
            self.stats['duplicates_filtered'] += 1
            return None
        self.processed_hashes.add(sig_hash)
        
        # æ¸…ç†è¿‡æœŸå“ˆå¸Œ
        if len(self.processed_hashes) > 10000:
            self.processed_hashes = set(list(self.processed_hashes)[-5000:])
        
        # èšåˆ
        symbol = signal.symbol.upper()
        self.symbol_signals[symbol].append(signal)
        
        # æ¸…ç†è¿‡æœŸä¿¡å·
        current_time = time.time()
        self.symbol_signals[symbol] = [
            s for s in self.symbol_signals[symbol]
            if current_time - s.timestamp < self.config['aggregation_window']
        ]
        
        # åˆå¹¶
        merged = self._merge_signals(self.symbol_signals[symbol])
        
        # æ›´æ–°æœ€ä½³ä¿¡å·
        existing_best = self.symbol_best_signal.get(symbol)
        if existing_best is None or merged.total_score > existing_best.total_score:
            self.symbol_best_signal[symbol] = merged
        
        # åŠ å…¥ä¼˜å…ˆçº§é˜Ÿåˆ—
        priority = self._get_priority(merged)
        heapq.heappush(
            self.signal_queue,
            PrioritizedSignal(priority=priority, timestamp=current_time, signal=merged)
        )
        
        # é™åˆ¶é˜Ÿåˆ—å¤§å°
        while len(self.signal_queue) > self.config['max_queue_size']:
            heapq.heappop(self.signal_queue)
        
        return merged
    
    def get_next_signal(self) -> Optional[AlphaSignal]:
        """è·å–ä¸‹ä¸€ä¸ªæœ€é«˜ä¼˜å…ˆçº§ä¿¡å·"""
        if not self.signal_queue:
            return None
        
        item = heapq.heappop(self.signal_queue)
        signal = item.signal
        
        self.stats['signals_output'] += 1
        if signal.tier == SignalTier.TIER_S:
            self.stats['tier_s_output'] += 1
        elif signal.tier == SignalTier.TIER_A:
            self.stats['tier_a_output'] += 1
        
        return signal
    
    def get_batch(self, size: int = None) -> List[AlphaSignal]:
        """è·å–ä¸€æ‰¹ä¿¡å·"""
        size = size or self.config['batch_size']
        signals = []
        
        while len(signals) < size and self.signal_queue:
            signal = self.get_next_signal()
            if signal:
                signals.append(signal)
        
        return signals
    
    def get_best_signals(self) -> Dict[str, AlphaSignal]:
        """è·å–æ¯ä¸ªå¸ç§çš„æœ€ä½³ä¿¡å·"""
        return dict(self.symbol_best_signal)
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡"""
        merge_rate = (
            self.stats['signals_merged'] / self.stats['signals_generated'] * 100
            if self.stats['signals_generated'] > 0 else 0
        )
        
        return {
            **self.stats,
            'queue_size': len(self.signal_queue),
            'active_symbols': len(self.symbol_signals),
            'merge_rate': round(merge_rate, 1),
            'alpha_engine_stats': self.alpha_engine.stats,
        }
    
    async def run_consumer(self, input_stream: str = 'events:raw', output_stream: str = 'events:alpha'):
        """
        è¿è¡Œæ¶ˆè´¹è€…å¾ªç¯
        
        ä» Redis Stream è¯»å–äº‹ä»¶, å¤„ç†åè¾“å‡ºåˆ°å¦ä¸€ä¸ª Stream
        """
        logger.info(f"ğŸ“¡ å¼€å§‹æ¶ˆè´¹ {input_stream} -> {output_stream}")
        
        last_id = '0'
        
        while True:
            try:
                # è¯»å–äº‹ä»¶
                messages = self.redis.read_stream(input_stream, last_id=last_id, count=10, block=1000)
                
                if not messages:
                    await asyncio.sleep(0.1)
                    continue
                
                for msg_id, msg_data in messages:
                    last_id = msg_id
                    
                    # è§£æäº‹ä»¶
                    try:
                        event = json.loads(msg_data.get('event_data', '{}'))
                    except:
                        event = msg_data
                    
                    # å¤„ç†
                    signal = await self.process_event(event)
                    
                    if signal and signal.tier in (SignalTier.TIER_S, SignalTier.TIER_A):
                        # è¾“å‡ºé«˜ä¼˜å…ˆçº§ä¿¡å·
                        self.redis.push_event(output_stream, {
                            'signal': json.dumps(signal.to_dict()),
                            'timestamp': str(time.time()),
                        })
                        
                        logger.info(
                            f"âš¡ [{signal.tier.value}] {signal.symbol} | "
                            f"åˆ†æ•°:{signal.total_score:.0f} | "
                            f"{signal.trigger_reason}"
                        )
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æ¶ˆè´¹å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(1)
        
        await self.alpha_engine.close()
        logger.info("ğŸ“¡ Signal Aggregator å·²å…³é—­")
    
    async def close(self):
        await self.alpha_engine.close()


# ===== æµ‹è¯• =====
if __name__ == "__main__":
    async def test():
        agg = SignalAggregator()
        
        # æ¨¡æ‹Ÿå¤šæºäº‹ä»¶
        events = [
            {
                'source': 'social_telegram',
                'channel': 'bwenews',
                'exchange': 'binance',
                'raw_text': 'Binance will list NEWTOKEN/USDT',
                'symbol': 'NEWTOKEN',
            },
            {
                'source': 'rest_api',
                'exchange': 'okx',
                'raw_text': 'New listing: NEWTOKEN',
                'symbol': 'NEWTOKEN',
            },
            {
                'source': 'rest_api',
                'exchange': 'bybit',
                'raw_text': 'NEWTOKEN now available',
                'symbol': 'NEWTOKEN',
            },
        ]
        
        for event in events:
            signal = await agg.process_event(event)
            if signal:
                print(f"\nä¿¡å·: {signal.symbol} | ç­‰çº§: {signal.tier.value} | åˆ†æ•°: {signal.total_score}")
        
        print(f"\nç»Ÿè®¡: {agg.get_stats()}")
        
        # è·å–æ‰¹é‡ä¿¡å·
        batch = agg.get_batch(5)
        print(f"\næ‰¹é‡ä¿¡å·: {[s.symbol for s in batch]}")
        
        await agg.close()
    
    asyncio.run(test())

